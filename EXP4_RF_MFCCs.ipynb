{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c36babc5-cd79-47e9-b8f6-3c4d34c6c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Complex pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from complexPyTorch.complexLayers import *\n",
    "from complexPyTorch.complexFunctions import *\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Load Data\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import pathlib\n",
    "from scipy.spatial.distance import cdist\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# MFCCS\n",
    "from scipy.io import wavfile\n",
    "import scipy.fftpack as fft\n",
    "from scipy.signal import get_window\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import scipy as spp\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e4af326-ca42-4fa0-87cb-74b2cd35e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_val(model, X, y, k=5):\n",
    "    np.random.seed(42)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    splits = np.array_split(indices, k)\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        test_indices = splits[i]\n",
    "        train_indices = np.concatenate([splits[j] for j in range(k) if j != i])\n",
    "        X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "        X_test, y_test = X.iloc[test_indices], y.iloc[test_indices] \n",
    "        model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "        y_pred = model.predict(X_test.to_numpy())\n",
    "        accuracy = np.mean(y_pred == y_test.to_numpy())\n",
    "        accuracies.append(accuracy)   \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7388210-df5b-457d-88da-455ad157c934",
   "metadata": {},
   "source": [
    "# RF Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "801310ec-ff4a-4c17-b448-73b9a3163e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    '''\n",
    "    A class that implements Random Forest algorithm from scratch.\n",
    "\n",
    "    For more information, refer to https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a\n",
    "\n",
    "    Parameters:\n",
    "    ----------    \n",
    "    num_tree: int, default=5\n",
    "        The number of voting decision tree classifiers used for classification.\n",
    "\n",
    "    subsample_size: float, default=None\n",
    "        The proportion of the total training examples used to train each decision trees.\n",
    "\n",
    "    max_depth: int, default=None\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until, all leaves are the purest.\n",
    "\n",
    "    max_features: int, float, default=None\n",
    "        For each decision tree, at each split from parent node to child nodes, consider only 'max features' to find threshold split. \n",
    "        If float and <1, max_features take the proportion of the features in the dataset.\n",
    "\n",
    "    bootstrap: bool, default=True\n",
    "        Bootstrap sampling of training examples, with or without replacement. \n",
    "\n",
    "    random_state: int, default=None\n",
    "        Controls the randomness of the estimator. The features are always randomly permuted at each split in each decision tree, \n",
    "        and bootstrap sampling is randomly permuted.\n",
    "    '''\n",
    "    def __init__(self, num_trees=10, tree = None, subsample_size=None, max_depth=None, \n",
    "                 max_features=None, bootstrap=True, random_state=None):\n",
    "        self.num_trees = num_trees\n",
    "        self.tree = tree\n",
    "        self.subsample_size = subsample_size\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = random_state\n",
    "        # Will store individually trained decision trees\n",
    "        self.decision_trees = []\n",
    "\n",
    "    def sample(self, X, y, random_state):\n",
    "        n_rows, n_cols = X.shape\n",
    "        if self.subsample_size is None:\n",
    "            sample_size = n_rows\n",
    "        else:\n",
    "            sample_size = int(n_rows*self.subsample_size)\n",
    "        np.random.seed(random_state)\n",
    "        samples = np.random.choice(a=n_rows, size=sample_size, replace=self.bootstrap)\n",
    "        return X[samples], y[samples]\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if len(self.decision_trees) > 0:\n",
    "            self.decision_trees = []\n",
    "        if isinstance(X, pd.core.frame.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.core.series.Series):\n",
    "            y = y.values   \n",
    "        # Build each tree of the forest\n",
    "        num_built = 0\n",
    "        while num_built < self.num_trees:\n",
    "\n",
    "            clf = self.tree(\n",
    "                max_depth=self.max_depth\n",
    "            )\n",
    "            # Obtain data sample\n",
    "            _X, _y = self.sample(X, y, self.random_state)\n",
    "            # Train\n",
    "            clf.fit(_X, _y)\n",
    "            # Save the classifier\n",
    "            self.decision_trees.append(clf)\n",
    "            num_built += 1\n",
    "            if self.random_state is not None:\n",
    "                self.random_state += 1\n",
    "\n",
    "    def predict(self, X):\n",
    "        y = []\n",
    "        for tree in self.decision_trees:\n",
    "            y.append(tree.predict(X))\n",
    "        # Reshape so we can find the most common value\n",
    "        y = np.swapaxes(y, axis1=0, axis2=1)\n",
    "        # Use majority voting for the final prediction\n",
    "        predicted_classes = stats.mode(y,axis=1)[0].reshape(-1)\n",
    "        return predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8673c8-4a0d-41c6-9d69-be12ed5b47d0",
   "metadata": {},
   "source": [
    "# Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd27ca32-822f-4d44-ace4-95fd85fd7933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pop': 0, 'metal': 1, 'disco': 2, 'blues': 3, 'reggae': 4, 'classical': 5, 'rock': 6, 'hiphop': 7, 'country': 8, 'jazz': 9}\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"Data/train\"\n",
    "SAMPLE_RATE = 22050\n",
    "TRACK_DURATION = 30 # measured in seconds\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "genre_list = os.listdir(DATASET_PATH)\n",
    "if '.DS_Store' in genre_list: genre_list.remove('.DS_Store')\n",
    "genre_mappings = dict(zip(genre_list, range(len(genre_list))))\n",
    "print(genre_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3b214-ec85-4fcf-b3c5-2adae5f12eb1",
   "metadata": {},
   "source": [
    "# 1. Simple RF with Real Valued Frequency domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c4f4c3-5cbb-45c5-a2c8-f8f541054b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"Data/exp4_data/train_tff_mfcc.csv\")\n",
    "te_df = pd.read_csv(\"Data/exp4_data/test_tff_mfcc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e18e5bb3-01c4-47bc-a52b-261782dd7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X_train = tr_df.drop('label', axis=1)\n",
    "y_train = tr_df['label']\n",
    "X_test = te_df.drop('label', axis=1)\n",
    "y_test = te_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0635424b-bd25-4240-9f15-892222eae606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    \"\"\"\n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \"\"\"\n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(X <= thresh).flatten()\n",
    "        right_idx = np.argwhere(X > thresh).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        if parent_loss - child_loss == 0: self.flag_zero = True\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "\n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "\n",
    "                if score > split['score']:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "\n",
    "        #print(split['score'])\n",
    "        return split['feat'], split['thresh']\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if self._is_finished(depth):\n",
    "            try: \n",
    "                most_common_Label = np.argmax(np.bincount(y))\n",
    "            except ValueError as e:\n",
    "                most_common_Label = 0\n",
    "            return Node(value=most_common_Label)\n",
    "\n",
    "        # get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "\n",
    "        # grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "947ed77e-eaf5-4fcf-a53e-309691e81bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "accuracy = accuracy_score(y_test.to_numpy(), y_pred)\n",
    "print(f'Accuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a70daf81-0a96-4f1e-8f10-cc8128225d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.7397397397397397\n",
      "Fold 2 Accuracy: 0.7227227227227228\n",
      "Fold 3 Accuracy: 0.7242242242242243\n",
      "Fold 4 Accuracy: 0.7262262262262262\n",
      "Fold 5 Accuracy: 0.7237237237237237\n",
      "Mean Accuracy: 0.7273273273273274\n"
     ]
    }
   ],
   "source": [
    "# CV:\n",
    "merged_df = pd.concat([tr_df, te_df], axis=0)\n",
    "X = merged_df.drop('label', axis=1)\n",
    "y = merged_df['label']\n",
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "cv_results = custom_cross_val(model, X, y, k=5)\n",
    "for i, acc in enumerate(cv_results):\n",
    "    print(f'Fold {i+1} Accuracy: {acc}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbfc10b-762a-456e-9c51-308422e12d1b",
   "metadata": {},
   "source": [
    "# 2. Simple RF with Complex Valued Frequency Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "942cfdde-9fbd-4d2a-bf7b-d69294b9278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"Data/exp4_data/train_tff_mfcc_comp.csv\")\n",
    "te_df = pd.read_csv(\"Data/exp4_data/test_tff_mfcc_comp.csv\")\n",
    "\n",
    "def df_csv_complex(df):\n",
    "    result_df = df.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    result_df.iloc[:, :-1] = df.iloc[:, :-1].apply(lambda col: col.apply(\n",
    "        lambda val: torch.tensor((complex(val.strip('()'))), dtype=torch.complex64) ))\n",
    "    return result_df\n",
    "\n",
    "tr_df = df_csv_complex(tr_df)\n",
    "te_df = df_csv_complex(te_df)\n",
    "tr_df = tr_df.applymap(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "te_df = te_df.applymap(lambda x: x.numpy() if hasattr(x, 'numpy') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3701c095-d89a-4710-83f8-a5eea1bcb633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X_train = tr_df.drop('label', axis=1)\n",
    "y_train = tr_df['label']\n",
    "X_test = te_df.drop('label', axis=1)\n",
    "y_test = te_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59004586-64c0-40fa-b385-425ceaef9759",
   "metadata": {},
   "source": [
    "## 2.1 Compare only real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "46b841e8-ffa4-4121-8e58-1601d602cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(X <= thresh).flatten()\n",
    "        right_idx = np.argwhere(X > thresh).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "\n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "\n",
    "                if score > split['score']:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "\n",
    "        return split['feat'], split['thresh']\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if self._is_finished(depth):\n",
    "            try: \n",
    "                most_common_Label = np.argmax(np.bincount(y))\n",
    "            except ValueError as e:\n",
    "                most_common_Label = 0\n",
    "            return Node(value=most_common_Label)\n",
    "\n",
    "        # get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "\n",
    "        # grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3f5c6687-185c-4170-9cec-0032147d57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.453125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "accuracy = accuracy_score(y_test.to_numpy(), y_pred)\n",
    "print(f'Accuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e991967b-8912-4102-921f-6a0b36c99654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.5530530530530531\n",
      "Fold 2 Accuracy: 0.5495495495495496\n",
      "Fold 3 Accuracy: 0.5495495495495496\n",
      "Fold 4 Accuracy: 0.55005005005005\n",
      "Fold 5 Accuracy: 0.5335335335335335\n",
      "Mean Accuracy: 0.5471471471471472\n"
     ]
    }
   ],
   "source": [
    "# CV:\n",
    "merged_df = pd.concat([tr_df, te_df], axis=0)\n",
    "X = merged_df.drop('label', axis=1)\n",
    "y = merged_df['label']\n",
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "cv_results = custom_cross_val(model, X, y, k=5)\n",
    "for i, acc in enumerate(cv_results):\n",
    "    print(f'Fold {i+1} Accuracy: {acc}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e491b-2571-41ac-add5-f25f79fa6d00",
   "metadata": {},
   "source": [
    "## 2.2 Compare only magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9bde5dea-f1e1-47fa-863f-f2e07b81c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(np.abs(X) <= np.abs(thresh)).flatten()\n",
    "        right_idx = np.argwhere(np.abs(X) > np.abs(thresh)).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "\n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "\n",
    "                if np.abs(score) > np.abs(split['score']) if split['score'] != -1 else -1:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "\n",
    "        return split['feat'], split['thresh']\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if self._is_finished(depth):\n",
    "            try: \n",
    "                most_common_Label = np.argmax(np.bincount(y))\n",
    "            except ValueError as e:\n",
    "                most_common_Label = 0\n",
    "            return Node(value=most_common_Label)\n",
    "\n",
    "        # get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "\n",
    "        # grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2b0685a5-c2eb-4251-8be2-d3a0cb298c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.378125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "accuracy = accuracy_score(y_test.to_numpy(), y_pred)\n",
    "print(f'Accuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b110d140-399e-4a24-8a6c-887ed28e0291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.47147147147147145\n",
      "Fold 2 Accuracy: 0.45245245245245247\n",
      "Fold 3 Accuracy: 0.47097097097097096\n",
      "Fold 4 Accuracy: 0.476976976976977\n",
      "Fold 5 Accuracy: 0.4954954954954955\n",
      "Mean Accuracy: 0.47347347347347346\n"
     ]
    }
   ],
   "source": [
    "# CV:\n",
    "merged_df = pd.concat([tr_df, te_df], axis=0)\n",
    "X = merged_df.drop('label', axis=1)\n",
    "y = merged_df['label']\n",
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "cv_results = custom_cross_val(model, X, y, k=5)\n",
    "for i, acc in enumerate(cv_results):\n",
    "    print(f'Fold {i+1} Accuracy: {acc}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989b35c-c9bb-471c-958b-fda5e4f11a09",
   "metadata": {},
   "source": [
    "# 3. Simple RF with Complex Valued Frequency Domain Features alt MFCC extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9eb684a8-8cb2-4154-8355-06add85eb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"Data/exp4_data/train_tff_mfcc_comp_mod.csv\")\n",
    "te_df = pd.read_csv(\"Data/exp4_data/test_tff_mfcc_comp_mod.csv\")\n",
    "\n",
    "def df_csv_complex(df):\n",
    "    result_df = df.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    result_df.iloc[:, :-1] = df.iloc[:, :-1].apply(lambda col: col.apply(\n",
    "        lambda val: torch.tensor((complex(val.strip('()'))), dtype=torch.complex64) ))\n",
    "    return result_df\n",
    "\n",
    "tr_df = df_csv_complex(tr_df)\n",
    "te_df = df_csv_complex(te_df)\n",
    "tr_df = tr_df.applymap(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "te_df = te_df.applymap(lambda x: x.numpy() if hasattr(x, 'numpy') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb690ab1-bb60-42b3-a6fe-1116cf92f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X_train = tr_df.drop('label', axis=1)\n",
    "y_train = tr_df['label']\n",
    "X_test = te_df.drop('label', axis=1)\n",
    "y_test = te_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a74a4-7e9a-4f07-873a-ed2d7521095a",
   "metadata": {},
   "source": [
    "## 3.1 Compare real only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cc65a4a-5bcc-46d6-b22f-12302377467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(X <= thresh).flatten()\n",
    "        right_idx = np.argwhere(X > thresh).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "\n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "\n",
    "                if score > split['score']:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "\n",
    "        return split['feat'], split['thresh']\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if self._is_finished(depth):\n",
    "            try: \n",
    "                most_common_Label = np.argmax(np.bincount(y))\n",
    "            except ValueError as e:\n",
    "                most_common_Label = 0\n",
    "            return Node(value=most_common_Label)\n",
    "\n",
    "        # get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "\n",
    "        # grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13c20dfc-d0c5-40dd-9481-44568b94bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "accuracy = accuracy_score(y_test.to_numpy(), y_pred)\n",
    "print(f'Accuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "837ae347-00e8-48e0-be4b-9d958de014d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.6941941941941941\n",
      "Fold 2 Accuracy: 0.7167167167167167\n",
      "Fold 3 Accuracy: 0.6981981981981982\n",
      "Fold 4 Accuracy: 0.6796796796796797\n",
      "Fold 5 Accuracy: 0.6936936936936937\n",
      "Mean Accuracy: 0.6964964964964965\n"
     ]
    }
   ],
   "source": [
    "# CV:\n",
    "merged_df = pd.concat([tr_df, te_df], axis=0)\n",
    "X = merged_df.drop('label', axis=1)\n",
    "y = merged_df['label']\n",
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "cv_results = custom_cross_val(model, X, y, k=5)\n",
    "for i, acc in enumerate(cv_results):\n",
    "    print(f'Fold {i+1} Accuracy: {acc}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5716a-aa4e-4e2f-adbd-038205620834",
   "metadata": {},
   "source": [
    "## 3.2 Compare magnitude only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033d7243-4acc-4dd8-b958-312b5beddec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(np.abs(X) <= np.abs(thresh)).flatten()\n",
    "        right_idx = np.argwhere(np.abs(X) > np.abs(thresh)).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "\n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "\n",
    "                if np.abs(score) > np.abs(split['score']) if split['score'] != -1 else -1:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "\n",
    "        return split['feat'], split['thresh']\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if self._is_finished(depth):\n",
    "            try: \n",
    "                most_common_Label = np.argmax(np.bincount(y))\n",
    "            except ValueError as e:\n",
    "                most_common_Label = 0\n",
    "            return Node(value=most_common_Label)\n",
    "\n",
    "        # get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "\n",
    "        # grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5328a-234b-4e09-9012-0a1a1fe910aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model = DecisionTree(max_depth=10)\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "accuracy = accuracy_score(y_test.to_numpy(), y_pred)\n",
    "print(f'Accuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f2105-ab5e-46ae-bb11-9123791342ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV:\n",
    "merged_df = pd.concat([tr_df, te_df], axis=0)\n",
    "X = merged_df.drop('label', axis=1)\n",
    "y = merged_df['label']\n",
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "cv_results = custom_cross_val(model, X, y, k=5)\n",
    "for i, acc in enumerate(cv_results):\n",
    "    print(f'Fold {i+1} Accuracy: {acc}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5eda5-22b1-40b2-bf87-b20c8a934dbc",
   "metadata": {},
   "source": [
    "# 4. RF: Complex Valued frequncy Domain + Real MFCC Coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2e6f0-6a72-436e-ac0a-c0c4a666eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"Data/exp4_data/train_tff_mfcc_comp.csv\")\n",
    "te_df = pd.read_csv(\"Data/exp4_data/test_tff_mfcc_comp.csv\")\n",
    "tr_df_mfcc = pd.read_csv(\"Data/exp4_data/train_tff_mfcc.csv\")\n",
    "te_df_mfcc = pd.read_csv(\"Data/exp4_data/test_tff_mfcc.csv\")\n",
    "\n",
    "def df_csv_complex(df):\n",
    "    result_df = df.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    result_df.iloc[:, :-1] = df.iloc[:, :-1].apply(lambda col: col.apply(\n",
    "        lambda val: torch.tensor((complex(val.strip('()'))), dtype=torch.complex64) ))\n",
    "    return result_df\n",
    "\n",
    "tr_df = df_csv_complex(tr_df)\n",
    "te_df = df_csv_complex(te_df)\n",
    "\n",
    "for i in [\"mean\", \"var\"]:\n",
    "    for j in range(1, 17):\n",
    "        tr_df[f\"mfcc_{j}_{i}\"] = tr_df_mfcc[f\"mfcc_{j}_{i}\"] \n",
    "        te_df[f\"mfcc_{j}_{i}\"] = te_df_mfcc[f\"mfcc_{j}_{i}\"] \n",
    "        \n",
    "tr_df = tr_df.applymap(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "te_df = te_df.applymap(lambda x: x.numpy() if hasattr(x, 'numpy') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb493bd-34bd-4d03-80af-0976a0f25638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X_train = tr_df.drop('label', axis=1)\n",
    "y_train = tr_df['label']\n",
    "X_test = te_df.drop('label', axis=1)\n",
    "y_test = te_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef6295-3eed-4fb6-95d4-67eb9b20ba39",
   "metadata": {},
   "source": [
    "## 4.1 Compare real Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1318d37-9250-4809-be8d-849bfbbcdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(X <= thresh).flatten()\n",
    "        right_idx = np.argwhere(X > thresh).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "\n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "\n",
    "                if score > split['score']:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "\n",
    "        return split['feat'], split['thresh']\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if self._is_finished(depth):\n",
    "            try: \n",
    "                most_common_Label = np.argmax(np.bincount(y))\n",
    "            except ValueError as e:\n",
    "                most_common_Label = 0\n",
    "            return Node(value=most_common_Label)\n",
    "\n",
    "        # get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "\n",
    "        # grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95b883-301b-4b05-84cd-c4de0d224190",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "accuracy = accuracy_score(y_test.to_numpy(), y_pred)\n",
    "print(f'Accuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0874b19-3cf7-41d3-8a49-2e3ab5aa3f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV:\n",
    "merged_df = pd.concat([tr_df, te_df], axis=0)\n",
    "X = merged_df.drop('label', axis=1)\n",
    "y = merged_df['label']\n",
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "cv_results = custom_cross_val(model, X, y, k=5)\n",
    "for i, acc in enumerate(cv_results):\n",
    "    print(f'Fold {i+1} Accuracy: {acc}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc72774-7088-4874-8b75-56c5310fa1c5",
   "metadata": {},
   "source": [
    "## 4.2 Compare magnitude Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900413b-bec5-4b09-a507-2d3ce5345ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(np.abs(X) <= np.abs(thresh)).flatten()\n",
    "        right_idx = np.argwhere(np.abs(X) > np.abs(thresh)).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "\n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "\n",
    "                if np.abs(score) > np.abs(split['score']) if split['score'] != -1 else -1:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "\n",
    "        return split['feat'], split['thresh']\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if self._is_finished(depth):\n",
    "            try: \n",
    "                most_common_Label = np.argmax(np.bincount(y))\n",
    "            except ValueError as e:\n",
    "                most_common_Label = 0\n",
    "            return Node(value=most_common_Label)\n",
    "\n",
    "        # get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "\n",
    "        # grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a7704-fccd-4a80-811e-e26503ab8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "accuracy = accuracy_score(y_test.to_numpy(), y_pred)\n",
    "print(f'Accuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9ac7f-f5bd-4f6f-aa06-2aff5c1f0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV:\n",
    "merged_df = pd.concat([tr_df, te_df], axis=0)\n",
    "X = merged_df.drop('label', axis=1)\n",
    "y = merged_df['label']\n",
    "np.random.seed(42)\n",
    "model = RandomForest(tree = DecisionTree, max_depth=10)\n",
    "cv_results = custom_cross_val(model, X, y, k=5)\n",
    "for i, acc in enumerate(cv_results):\n",
    "    print(f'Fold {i+1} Accuracy: {acc}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_results)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
