{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fbb233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"A Multilayer Perceptron class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers=[3, 3], num_outputs=2, act_f = None):\n",
    "        \"\"\"Constructor for the MLP. Takes the number of inputs,\n",
    "            a variable number of hidden layers, and number of outputs\n",
    "\n",
    "        Args:\n",
    "            num_inputs (int): Number of inputs\n",
    "            hidden_layers (list): A list of ints for the hidden layers\n",
    "            num_outputs (int): Number of outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "        \n",
    "        # Activation functions for each layer\n",
    "        if act_f == None: self.act_f = [\"sigmoid\"]*(len(layers) - 1)\n",
    "        else: self.act_f = act_f\n",
    "\n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i + 1])\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "\n",
    "        # save derivatives per layer\n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            d = np.zeros((layers[i], layers[i + 1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "\n",
    "        # save activations per layer\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        \"\"\"Computes forward propagation of the network based on input signals.\n",
    "\n",
    "        Args:\n",
    "            inputs (ndarray): Input signals\n",
    "        Returns:\n",
    "            activations (ndarray): Output values\n",
    "        \"\"\"\n",
    "\n",
    "        # the input layer activation is just the input itself\n",
    "        activations = inputs\n",
    "\n",
    "        # save the activations for backpropogation\n",
    "        self.activations[0] = activations\n",
    "\n",
    "        # iterate through the network layers\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # calculate matrix multiplication between previous activation and weight matrix\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # apply sigmoid activation function\n",
    "            cur_act_f = self.map_to_act(self.act_f[i])\n",
    "            activations = cur_act_f(net_inputs)\n",
    "\n",
    "            # save the activations for backpropogation\n",
    "            self.activations[i + 1] = activations\n",
    "\n",
    "        # return output layer activation\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_propagate(self, error):\n",
    "        \"\"\"Backpropogates an error signal.\n",
    "        Args:\n",
    "            error (ndarray): The error to backprop.\n",
    "        Returns:\n",
    "            error (ndarray): The final error of the input\n",
    "        \"\"\"\n",
    "\n",
    "        # iterate backwards through the network layers\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "\n",
    "            # get activation for previous layer\n",
    "            activations = self.activations[i+1]\n",
    "\n",
    "            # apply sigmoid derivative function\n",
    "            # apply sigmoid activation function\n",
    "            cur_act_f_der = self.map_to_act_derivative(self.act_f[i])\n",
    "            delta = error * cur_act_f_der(activations)\n",
    "\n",
    "            # reshape delta as to have it as a 2d array\n",
    "            delta_re = delta.reshape(delta.shape[0], -1).T\n",
    "\n",
    "            # get activations for current layer\n",
    "            current_activations = self.activations[i]\n",
    "\n",
    "            # reshape activations as to have them as a 2d column matrix\n",
    "            current_activations = current_activations.reshape(current_activations.shape[0],-1)\n",
    "\n",
    "            # save derivative after applying matrix multiplication\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_re)\n",
    "\n",
    "            # backpropogate the next error\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        \"\"\"Trains model running forward prop and backprop\n",
    "        Args:\n",
    "            inputs (ndarray): X\n",
    "            targets (ndarray): Y\n",
    "            epochs (int): Num. epochs we want to train the network for\n",
    "            learning_rate (float): Step to apply to gradient descent\n",
    "        \"\"\"\n",
    "        # now enter the training loop\n",
    "        for i in range(epochs):\n",
    "            sum_errors = 0\n",
    "\n",
    "            # iterate through all the training data\n",
    "            for j, input in enumerate(inputs):\n",
    "                target = targets[j]\n",
    "\n",
    "                # activate the network!\n",
    "                output = self.forward_propagate(input)\n",
    "\n",
    "                error = target - output\n",
    "\n",
    "                self.back_propagate(error)\n",
    "\n",
    "                # now perform gradient descent on the derivatives\n",
    "                # (this will update the weights\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                # keep track of the MSE for reporting later\n",
    "                sum_errors += self._mse(target, output)\n",
    "\n",
    "            # Epoch complete, report the training error\n",
    "            print(\"Error: {} at epoch {}\".format(sum_errors / len(items), i+1))\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        print(\"=====\")\n",
    "\n",
    "\n",
    "    def gradient_descent(self, learningRate=1):\n",
    "        \"\"\"Learns by descending the gradient\n",
    "        Args:\n",
    "            learningRate (float): How fast to learn.\n",
    "        \"\"\"\n",
    "        # update the weights by stepping down the gradient\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights += derivatives * learningRate\n",
    "            \n",
    "    def map_to_act(self, name):\n",
    "        if name == \"sigmoid\":\n",
    "            return lambda x: self._sigmoid(x)\n",
    "        if name == \"linear\":\n",
    "            return lambda x: self._linear(x)\n",
    "        if name == \"relu\":\n",
    "            return lambda x: self._relu(x)\n",
    "        if name == \"tanh\":\n",
    "            return lambda x: self._tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"No such activation function found\")\n",
    "            \n",
    "    def map_to_act_derivative(self, name):\n",
    "        if name == \"sigmoid\":\n",
    "            return lambda x: self._sigmoid_derivative(x)\n",
    "        if name == \"linear\":\n",
    "            return lambda x: self._linear_derivative(x)\n",
    "        if name == \"relu\":\n",
    "            return lambda x: self._relu_derivative(x)\n",
    "        if name == \"tanh\":\n",
    "            return lambda x: self._tanh_derivative(x)\n",
    "        else:\n",
    "            raise ValueError(\"No such activation function found\")\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        y = 1.0 / (1 + np.exp(-x))\n",
    "        return y\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)\n",
    "    \n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def _linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def _linear_derivative(self, x):\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def _mse(self, target, output):\n",
    "        \"\"\"Mean Squared Error loss function\n",
    "        Args:\n",
    "            target (ndarray): The ground trut\n",
    "            output (ndarray): The predicted values\n",
    "        Returns:\n",
    "            (float): Output\n",
    "        \"\"\"\n",
    "        return np.average((target - output) ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4321d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0004678107787784005 at epoch 1\n",
      "Error: 6.095544072771768e-10 at epoch 2\n",
      "Error: 1.351024188843233e-12 at epoch 3\n",
      "Error: 2.9945283699442304e-15 at epoch 4\n",
      "Error: 6.6373456605878654e-18 at epoch 5\n",
      "Error: 1.4711620467724028e-20 at epoch 6\n",
      "Error: 3.2608587950938983e-23 at epoch 7\n",
      "Error: 7.229003979558104e-26 at epoch 8\n",
      "Error: 1.5781609711291676e-28 at epoch 9\n",
      "Error: 4.180766954670143e-31 at epoch 10\n",
      "Error: 7.943251596777583e-32 at epoch 11\n",
      "Error: 4.967677494710597e-32 at epoch 12\n",
      "Error: 3.9201304941843536e-32 at epoch 13\n",
      "Error: 3.6600303450027655e-32 at epoch 14\n",
      "Error: 3.434946912402229e-32 at epoch 15\n",
      "Error: 3.168605245568057e-32 at epoch 16\n",
      "Error: 3.0013382298980256e-32 at epoch 17\n",
      "Error: 2.8928876100968116e-32 at epoch 18\n",
      "Error: 2.748707679635122e-32 at epoch 19\n",
      "Error: 2.764650768456003e-32 at epoch 20\n",
      "Error: 2.775147086652913e-32 at epoch 21\n",
      "Error: 2.794483423294561e-32 at epoch 22\n",
      "Error: 2.7638996557776916e-32 at epoch 23\n",
      "Error: 2.754115931660205e-32 at epoch 24\n",
      "Error: 2.735781078589638e-32 at epoch 25\n",
      "Error: 2.71883289507903e-32 at epoch 26\n",
      "Error: 2.7043499018972385e-32 at epoch 27\n",
      "Error: 2.655046095320925e-32 at epoch 28\n",
      "Error: 2.64402977603903e-32 at epoch 29\n",
      "Error: 2.62140009919248e-32 at epoch 30\n",
      "Error: 2.6077645151862184e-32 at epoch 31\n",
      "Error: 2.589583736511203e-32 at epoch 32\n",
      "Error: 2.582443351242387e-32 at epoch 33\n",
      "Error: 2.5722744411360227e-32 at epoch 34\n",
      "Error: 2.5564047783942716e-32 at epoch 35\n",
      "Error: 2.55002995027835e-32 at epoch 36\n",
      "Error: 2.538166221820925e-32 at epoch 37\n",
      "Error: 2.53252324708387e-32 at epoch 38\n",
      "Error: 2.524742490108546e-32 at epoch 39\n",
      "Error: 2.526129159668505e-32 at epoch 40\n",
      "Error: 2.5058683766535506e-32 at epoch 41\n",
      "Error: 2.4952179840610893e-32 at epoch 42\n",
      "Error: 2.482583883625909e-32 at epoch 43\n",
      "Error: 2.468717188026321e-32 at epoch 44\n",
      "Error: 2.4621690262154044e-32 at epoch 45\n",
      "Error: 2.4535408600645495e-32 at epoch 46\n",
      "Error: 2.431739333094086e-32 at epoch 47\n",
      "Error: 2.4169481911211921e-32 at epoch 48\n",
      "Error: 2.413943740407948e-32 at epoch 49\n",
      "Error: 2.409244471343643e-32 at epoch 50\n",
      "Training complete!\n",
      "=====\n",
      "\n",
      "Our network believes that 3.0 + 0.1 is equal to 3.1000000000000028\n"
     ]
    }
   ],
   "source": [
    "items = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "targets = np.array([[i[0] + i[1]] for i in items])\n",
    "\n",
    "mlp = MLP(2, [5], 1, [\"linear\", \"linear\"])\n",
    "\n",
    "mlp.train(items, targets, 50, 0.1)\n",
    "\n",
    "input = np.array([3, 0.1])\n",
    "target = np.array([0.4])\n",
    "\n",
    "output = mlp.forward_propagate(input)\n",
    "\n",
    "print()\n",
    "print(\"Our network believes that {} + {} is equal to {}\".format(input[0], input[1], output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256364e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b81b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6a74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8b73f96",
   "metadata": {},
   "source": [
    "## Complex Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20dcdb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"A Multilayer Perceptron class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers=[3, 3], num_outputs=2, act_f = None):\n",
    "        \"\"\"Constructor for the MLP. Takes the number of inputs,\n",
    "            a variable number of hidden layers, and number of outputs\n",
    "\n",
    "        Args:\n",
    "            num_inputs (int): Number of inputs\n",
    "            hidden_layers (list): A list of ints for the hidden layers\n",
    "            num_outputs (int): Number of outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "        \n",
    "        # Activation functions for each layer\n",
    "        if act_f == None: self.act_f = [\"sigmoid\"]*(len(layers) - 1)\n",
    "        else: self.act_f = act_f\n",
    "\n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i + 1]) * np.exp(1j*np.pi / 2)\n",
    "            #w = np.random.rand(layers[i], layers[i + 1]) + 1j*np.random.rand(layers[i], layers[i + 1])\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "\n",
    "        # save derivatives per layer\n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            d = np.zeros((layers[i], layers[i + 1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "\n",
    "        # save activations per layer\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        \"\"\"Computes forward propagation of the network based on input signals.\n",
    "\n",
    "        Args:\n",
    "            inputs (ndarray): Input signals\n",
    "        Returns:\n",
    "            activations (ndarray): Output values\n",
    "        \"\"\"\n",
    "\n",
    "        # the input layer activation is just the input itself\n",
    "        activations = inputs\n",
    "\n",
    "        # save the activations for backpropogation\n",
    "        self.activations[0] = activations\n",
    "\n",
    "        # iterate through the network layers\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # calculate matrix multiplication between previous activation and weight matrix\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # apply sigmoid activation function\n",
    "            cur_act_f = self.map_to_act(self.act_f[i])\n",
    "            activations = cur_act_f(net_inputs)\n",
    "\n",
    "            # save the activations for backpropogation\n",
    "            self.activations[i + 1] = activations\n",
    "\n",
    "        # return output layer activation\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_propagate(self, error):\n",
    "        \"\"\"Backpropogates an error signal.\n",
    "        Args:\n",
    "            error (ndarray): The error to backprop.\n",
    "        Returns:\n",
    "            error (ndarray): The final error of the input\n",
    "        \"\"\"\n",
    "\n",
    "        # iterate backwards through the network layers\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "\n",
    "            # get activation for previous layer\n",
    "            activations = self.activations[i+1]\n",
    "\n",
    "            # apply sigmoid derivative function\n",
    "            # apply sigmoid activation function\n",
    "            cur_act_f_der = self.map_to_act_derivative(self.act_f[i])\n",
    "            delta = error * cur_act_f_der(activations)\n",
    "\n",
    "            # reshape delta as to have it as a 2d array\n",
    "            delta_re = delta.reshape(delta.shape[0], -1).T\n",
    "\n",
    "            # get activations for current layer\n",
    "            current_activations = self.activations[i]\n",
    "\n",
    "            # reshape activations as to have them as a 2d column matrix\n",
    "            current_activations = current_activations.reshape(current_activations.shape[0],-1)\n",
    "\n",
    "            # save derivative after applying matrix multiplication\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_re)\n",
    "\n",
    "            # backpropogate the next error\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        \"\"\"Trains model running forward prop and backprop\n",
    "        Args:\n",
    "            inputs (ndarray): X\n",
    "            targets (ndarray): Y\n",
    "            epochs (int): Num. epochs we want to train the network for\n",
    "            learning_rate (float): Step to apply to gradient descent\n",
    "        \"\"\"\n",
    "        # now enter the training loop\n",
    "        for i in range(epochs):\n",
    "            sum_errors = 0\n",
    "\n",
    "            # iterate through all the training data\n",
    "            for j, input in enumerate(inputs):\n",
    "                target = targets[j]\n",
    "\n",
    "                # activate the network!\n",
    "                output = self.forward_propagate(input)\n",
    "\n",
    "                error = target - output\n",
    "\n",
    "                self.back_propagate(error)\n",
    "\n",
    "                # now perform gradient descent on the derivatives\n",
    "                # (this will update the weights\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                # keep track of the MSE for reporting later\n",
    "                sum_errors += self._mse(target, output)\n",
    "\n",
    "            # Epoch complete, report the training error\n",
    "            print(\"Error: {} at epoch {}\".format(sum_errors / len(items), i+1))\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        print(\"=====\")\n",
    "\n",
    "\n",
    "    def gradient_descent(self, learningRate=1):\n",
    "        \"\"\"Learns by descending the gradient\n",
    "        Args:\n",
    "            learningRate (float): How fast to learn.\n",
    "        \"\"\"\n",
    "        # update the weights by stepping down the gradient\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights += derivatives * learningRate\n",
    "            \n",
    "    def map_to_act(self, name):\n",
    "        if name == \"sigmoid\":\n",
    "            return lambda x: self._sigmoid(x)\n",
    "        if name == \"linear\":\n",
    "            return lambda x: self._linear(x)\n",
    "        if name == \"relu\":\n",
    "            return lambda x: self._relu(x)\n",
    "        if name == \"tanh\":\n",
    "            return lambda x: self._tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"No such activation function found\")\n",
    "            \n",
    "    def map_to_act_derivative(self, name):\n",
    "        if name == \"sigmoid\":\n",
    "            return lambda x: self._sigmoid_derivative(x)\n",
    "        if name == \"linear\":\n",
    "            return lambda x: self._linear_derivative(x)\n",
    "        if name == \"relu\":\n",
    "            return lambda x: self._relu_derivative(x)\n",
    "        if name == \"tanh\":\n",
    "            return lambda x: self._tanh_derivative(x)\n",
    "        else:\n",
    "            raise ValueError(\"No such activation function found\")\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        y = 1.0 / (1 + np.exp(-x))\n",
    "        return y\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)\n",
    "    \n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def _linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def _linear_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 1)\n",
    "\n",
    "\n",
    "    def _mse(self, target, output):\n",
    "        \"\"\"Mean Squared Error loss function\n",
    "        Args:\n",
    "            target (ndarray): The ground trut\n",
    "            output (ndarray): The predicted values\n",
    "        Returns:\n",
    "            (float): Output\n",
    "        \"\"\"\n",
    "        return np.abs(np.average((target - output) ** 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f5f1ad1d-d5cd-428c-b048-a4fdbb2ef6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.26425343e-17+0.20646825j, 2.53538736e-17+0.41406018j,\n",
       "        5.17136241e-17+0.84454757j, 4.70742204e-17+0.76878036j,\n",
       "        1.34806250e-17+0.22015531j],\n",
       "       [5.97517210e-17+0.97581966j, 4.45270098e-17+0.72718125j,\n",
       "        3.85220473e-18+0.06291128j, 2.92420291e-17+0.47755858j,\n",
       "        3.69145269e-18+0.060286j  ],\n",
       "       [3.78665571e-17+0.61840781j, 5.00101128e-17+0.81672712j,\n",
       "        2.65342407e-17+0.43333704j, 1.36885519e-17+0.22355102j,\n",
       "        1.50939264e-17+0.24650252j],\n",
       "       [5.49037087e-17+0.89664561j, 1.07507449e-17+0.17557299j,\n",
       "        2.78705517e-17+0.45516065j, 3.32914645e-17+0.54369087j,\n",
       "        4.34645428e-17+0.70982985j],\n",
       "       [1.12621893e-17+0.18392551j, 4.51077465e-17+0.73666541j,\n",
       "        1.73444178e-17+0.28325584j, 3.77989413e-17+0.61730356j,\n",
       "        2.19195316e-17+0.35797312j],\n",
       "       [5.57210350e-17+0.90999356j, 2.95346112e-17+0.48233681j,\n",
       "        4.34562373e-17+0.70969421j, 3.62390404e-17+0.59182844j,\n",
       "        9.21318667e-18+0.15046276j],\n",
       "       [6.69716280e-18+0.10937297j, 3.82413240e-17+0.62452822j,\n",
       "        1.16657555e-17+0.19051625j, 1.03040877e-18+0.01682785j,\n",
       "        3.18794874e-17+0.52063154j],\n",
       "       [1.78318404e-17+0.29121605j, 1.91233488e-17+0.31230799j,\n",
       "        7.85763799e-18+0.12832497j, 3.26979790e-17+0.53399852j,\n",
       "        1.01775321e-17+0.16621171j],\n",
       "       [3.98026400e-17+0.65002644j, 7.61187944e-18+0.12431143j,\n",
       "        2.48325999e-18+0.04055471j, 1.95997720e-17+0.32008857j,\n",
       "        5.64110247e-17+0.92126195j],\n",
       "       [1.01553859e-17+0.16585004j, 5.92568865e-17+0.9677384j ,\n",
       "        2.11428224e-17+0.34528849j, 1.60452927e-18+0.02620395j,\n",
       "        1.99859665e-17+0.3263956j ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(2*np.exp(1j*np.pi / 2))\n",
    "res = np.random.rand(10, 5)*np.exp(1j*np.pi / 2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0a2cc581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.016113556066521102 at epoch 1\n",
      "Error: 0.0017091393552608365 at epoch 2\n",
      "Error: 1.6883297175821061e-06 at epoch 3\n",
      "Error: 1.6212053120294535e-09 at epoch 4\n",
      "Error: 1.5005733251427388e-12 at epoch 5\n",
      "Error: 1.3854451022504825e-15 at epoch 6\n",
      "Error: 1.2791029580993921e-18 at epoch 7\n",
      "Error: 1.180929363703168e-21 at epoch 8\n",
      "Error: 1.0902985048689648e-24 at epoch 9\n",
      "Error: 1.0088334357685943e-27 at epoch 10\n",
      "Error: 1.0604484261338763e-30 at epoch 11\n",
      "Error: 3.0918723615793123e-32 at epoch 12\n",
      "Error: 2.595034984076604e-32 at epoch 13\n",
      "Error: 2.7007338249432697e-32 at epoch 14\n",
      "Error: 2.2114197444764203e-32 at epoch 15\n",
      "Error: 2.2290068353711644e-32 at epoch 16\n",
      "Error: 2.2139213840263765e-32 at epoch 17\n",
      "Error: 2.2108927728417667e-32 at epoch 18\n",
      "Error: 2.3248897483453608e-32 at epoch 19\n",
      "Error: 2.1297909111056844e-32 at epoch 20\n",
      "Error: 1.9612609381500867e-32 at epoch 21\n",
      "Error: 1.9806098018980565e-32 at epoch 22\n",
      "Error: 2.12487121470975e-32 at epoch 23\n",
      "Error: 1.945691706072306e-32 at epoch 24\n",
      "Error: 2.410407571788697e-32 at epoch 25\n",
      "Error: 2.0065573366130952e-32 at epoch 26\n",
      "Error: 1.979709719833193e-32 at epoch 27\n",
      "Error: 2.106502282340646e-32 at epoch 28\n",
      "Error: 1.9356611296483282e-32 at epoch 29\n",
      "Error: 2.0839317779430276e-32 at epoch 30\n",
      "Error: 1.9505977624311532e-32 at epoch 31\n",
      "Error: 2.1605561356910738e-32 at epoch 32\n",
      "Error: 1.9648394954216266e-32 at epoch 33\n",
      "Error: 2.1161421850883403e-32 at epoch 34\n",
      "Error: 2.0104613871230438e-32 at epoch 35\n",
      "Error: 2.08759366803197e-32 at epoch 36\n",
      "Error: 2.0108886416285265e-32 at epoch 37\n",
      "Error: 2.0692160070849322e-32 at epoch 38\n",
      "Error: 2.1674891723672418e-32 at epoch 39\n",
      "Error: 1.9307415232876428e-32 at epoch 40\n",
      "Error: 1.9109810614070178e-32 at epoch 41\n",
      "Error: 2.0594481445838877e-32 at epoch 42\n",
      "Error: 2.1636228942793503e-32 at epoch 43\n",
      "Error: 1.931505269368292e-32 at epoch 44\n",
      "Error: 2.0879651756092733e-32 at epoch 45\n",
      "Error: 2.0332748994894318e-32 at epoch 46\n",
      "Error: 2.0388042591338966e-32 at epoch 47\n",
      "Error: 2.0837105543970018e-32 at epoch 48\n",
      "Error: 2.1633454023597683e-32 at epoch 49\n",
      "Error: 2.0863541984071098e-32 at epoch 50\n",
      "Training complete!\n",
      "=====\n",
      "\n",
      "Our network believes that 3.0 + 0.1 is equal to (3.100000000000002+1.890412666158187e-15j)\n"
     ]
    }
   ],
   "source": [
    "items = np.array([[random()/1.9 for _ in range(2)] for _ in range(1000)])\n",
    "targets = np.array([[i[0] + i[1]] for i in items])\n",
    "\n",
    "mlp = MLP(2, [10, 5], 1, [\"linear\", \"linear\", \"linear\"])\n",
    "\n",
    "mlp.train(items, targets, 50, 0.1)\n",
    "\n",
    "input = np.array([3, 0.1])\n",
    "target = np.array([0.3])\n",
    "\n",
    "output = mlp.forward_propagate(input)\n",
    "\n",
    "print()\n",
    "print(\"Our network believes that {} + {} is equal to {}\".format(input[0], input[1], output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c256ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: nan at epoch 1\n",
      "Error: nan at epoch 2\n",
      "Error: nan at epoch 3\n",
      "Error: nan at epoch 4\n",
      "Error: nan at epoch 5\n",
      "Error: nan at epoch 6\n",
      "Error: nan at epoch 7\n",
      "Error: nan at epoch 8\n",
      "Error: nan at epoch 9\n",
      "Error: nan at epoch 10\n",
      "Error: nan at epoch 11\n",
      "Error: nan at epoch 12\n",
      "Error: nan at epoch 13\n",
      "Error: nan at epoch 14\n",
      "Error: nan at epoch 15\n",
      "Error: nan at epoch 16\n",
      "Error: nan at epoch 17\n",
      "Error: nan at epoch 18\n",
      "Error: nan at epoch 19\n",
      "Error: nan at epoch 20\n",
      "Error: nan at epoch 21\n",
      "Error: nan at epoch 22\n",
      "Error: nan at epoch 23\n",
      "Error: nan at epoch 24\n",
      "Error: nan at epoch 25\n",
      "Error: nan at epoch 26\n",
      "Error: nan at epoch 27\n",
      "Error: nan at epoch 28\n",
      "Error: nan at epoch 29\n",
      "Error: nan at epoch 30\n",
      "Error: nan at epoch 31\n",
      "Error: nan at epoch 32\n",
      "Error: nan at epoch 33\n",
      "Error: nan at epoch 34\n",
      "Error: nan at epoch 35\n",
      "Error: nan at epoch 36\n",
      "Error: nan at epoch 37\n",
      "Error: nan at epoch 38\n",
      "Error: nan at epoch 39\n",
      "Error: nan at epoch 40\n",
      "Error: nan at epoch 41\n",
      "Error: nan at epoch 42\n",
      "Error: nan at epoch 43\n",
      "Error: nan at epoch 44\n",
      "Error: nan at epoch 45\n",
      "Error: nan at epoch 46\n",
      "Error: nan at epoch 47\n",
      "Error: nan at epoch 48\n",
      "Error: nan at epoch 49\n",
      "Error: nan at epoch 50\n",
      "Training complete!\n",
      "=====\n",
      "\n",
      "Our network believes that 3.0 + 0.1 is equal to (nan+nanj)\n"
     ]
    }
   ],
   "source": [
    "items = np.array([[random()/1.8 for _ in range(2)] for _ in range(1000)])\n",
    "targets = np.array([[i[0] + i[1]] for i in items])\n",
    "\n",
    "mlp = MLP(2, [10, 5], 1, [\"linear\", \"linear\", \"linear\"])\n",
    "\n",
    "mlp.train(items, targets, 50, 0.1)\n",
    "\n",
    "input = np.array([3, 0.1])\n",
    "target = np.array([0.3])\n",
    "\n",
    "output = mlp.forward_propagate(input)\n",
    "\n",
    "print()\n",
    "print(\"Our network believes that {} + {} is equal to {}\".format(input[0], input[1], output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193de72-5d18-4744-8c00-b99c6687a9b5",
   "metadata": {},
   "source": [
    "NaNs are observed because complex values easily explode on multiplication as long as the weights have magnitude strictly less than one. Very sensitve, even a factor of 0.1 causes relsults to be drastcially altered.\n",
    "$$a \\cdot c = ac$$\n",
    "$$(a + bi) \\cdot (c + di) = (ac - bd) + (bc + ad)i \\implies |(a + bi) \\cdot (c + di)| = \\sqrt{(ac - bd)^2 + (bc + ad)^2}$$\n",
    "The weight updates can drastically alter their magnitude shooting up the forward output to extremely large values.\n",
    "\n",
    "Solutions: \n",
    "1. A bi-segmented network architecture that considers independet treatment of magnitude and phase as well as their interaction (similar to a graph model?).\n",
    "2. Keeping weights always real, but difficlut treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c665558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c5046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e46c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
